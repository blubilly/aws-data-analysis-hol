{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glue Job Basic Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이번 Lab에서는 Glue Job의 기본 Template을 살펴보고 SageMaker notebook을 사용하여 간단하게 데이터를 읽고, ETL하고, Write하는 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3에 업로드한 데이터를 읽어오기 위해 각자 S3 bucket에 지정한 account-id를 account_id 변수에 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = ''\n",
    "s3_bucket = 's3://aws-glue-hol-' + account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제 Glue Job의 Template은 다음과 같습니다. (아래코드는 실행하지 말 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "# SparkContext 변수 생성\n",
    "sc = SparkContext()\n",
    "# GlueContext 변수 생성\n",
    "glueContext = GlueContext(sc)\n",
    "# SparkSession 변수 생성\n",
    "spark = glueContext.spark_session\n",
    "# Job 변수 생성\n",
    "job = Job(glueContext)\n",
    "# Job 초기화\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# S3에서 csv 데이터를 읽어 DynamicFrame 생성\n",
    "titanic_dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [s3_bucket + '/train']},\n",
    "    format='csv',\n",
    "    format_options={\n",
    "        \"withHeader\": True,\n",
    "        \"delimiter\": ','\n",
    "    })\n",
    "\n",
    "# Spark 활용: DynamicFrame을 DataFrame으로 변환 및 initail column을 추가\n",
    "titanic_csv_df = titanic_dyf.toDF()\n",
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))\n",
    "\n",
    "# Glue 활용: DataFrame을 DynamicFrame으로 변환하여 Name column 삭제\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "\n",
    "# json format으로 output 디렉토리에 저장\n",
    "titanic_csv_df.write \\\n",
    "    .format('json') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(s3_bucket + '/output')\n",
    "# Job commit\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook에서는 SparkContext(sc)와 SparkSession(spark)이 지원되므로 선언해서 사용하지 않아도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook에서는 아래와 같이 이미 선언된 SparkContext(sc)를 사용해서 GlueContext 생성이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Jupyter Notebook에서 이미 생성한 SparkContext(sc)를 사용해서 GlueContext 변수 생성\n",
    "glueContext = GlueContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 데이터를 읽어 DynamicFrame 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [s3_bucket + '/train']},\n",
    "    format='csv',\n",
    "    format_options={\n",
    "        'withHeader': True,\n",
    "        'delimiter': ','\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynamicFrame을 DataFrame으로 변환하고 데이터 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df = titanic_dyf.toDF()\n",
    "titanic_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name 컬럼에서 유의미한 String만 추출하여 initial 컬럼을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json format으로 output 디렉토리에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df.write \\\n",
    "    .format('json') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(s3_bucket + '/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 데이터 수를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 및 샘플 데이터 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial 컬럼 생성으로 Name 컬럼은 필요없으므로 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "titanic_csv_dyf.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Json으로 변환해서 저장된 데이터가 정상적으로 저장되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [s3_bucket + '/output']},\n",
    "    format='json').toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
